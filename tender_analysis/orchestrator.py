import asyncio
import re
import os
import json
import random
from typing import List, Dict, Any, Callable
import tiktoken
import uuid
import docx
from docx.oxml.text.paragraph import CT_P
from docx.oxml.table import CT_Tbl

from agents import Runner, RunConfig, ModelSettings, Agent
from openai.types.responses import ResponseTextDeltaEvent
from agents.extensions.models.litellm_model import LitellmModel
from fastmcp import Client as MCPClient

from tender_analysis.core.config import settings
from tender_analysis.analysis_agents import (
    business_requirement_extractor_agent,
    technical_requirement_extractor_agent,
    pricing_requirement_extractor_agent,
    scoring_requirement_extractor_agent,
    standard_template_extractor_agent,
    non_standard_template_extractor_agent,
    checklist_outline_agent,
    checklist_enrichment_agent,
    project_summary_agent,
)

# ==============================================================================
# å…¨å±€é…ç½®åŒº
# ==============================================================================

# ========================================
# ğŸ”¥ æ ¸å¿ƒè¾“å…¥æ–‡ä»¶é…ç½®
# ========================================
# DEFAULT_DOCX_PATH: æ‹›æ ‡æ–‡ä»¶Wordæ–‡æ¡£çš„ç»å¯¹è·¯å¾„
#   - è¿™æ˜¯æ•´ä¸ªæ ‡ä¹¦è§£ææµç¨‹çš„èµ·ç‚¹
#   - ä¿®æ”¹è¿™é‡Œå¯ä»¥åˆ‡æ¢è¦åˆ†æçš„æ‹›æ ‡æ–‡ä»¶
#   - ä¹Ÿå¯ä»¥é€šè¿‡APIå‚æ•°åŠ¨æ€ä¼ å…¥
# ========================================
DEFAULT_DOCX_PATH = "/Users/cris/Documents/JR/Agent_py/TenderBot_New/jr_tenderbot_mcp/mcp-file/data/ã€ä¸­å›½ä¸Šå¸‚å…¬å¸åä¼šã€‘æ ‡ä¹¦251127.converted.docx"
DEFAULT_MODEL_NAME = "gpt-4.1-mini"
MAX_TOKENS_PER_CHUNK = 45000  # ç”¨äºæ–‡æœ¬åˆ†å—çš„ Token é˜ˆå€¼
SUMMARY_INPUT_CHAR_LIMIT = 20000  # é¡¹ç›®æ¦‚è¦é˜¶æ®µè¾“å…¥æˆªæ–­é˜ˆå€¼

# å®šä¹‰äº†æ‰€æœ‰ä¸­é—´åŠæœ€ç»ˆäº§å‡ºæ–‡ä»¶çš„æ ‡å‡†æ–‡ä»¶å
OUTPUT_PATHS = {
    "business": "business_summary.md",
    "technical": "technical_summary.md",
    "pricing": "pricing_summary.md",
    "scoring": "scoring_summary.md",
    "template": "templates.json",
    "intermediate_md": "intermediate_full.md",
    "intermediate_chunks": "intermediate_chunks.json",
    "checklist_outline": "checklist_outline.md",
    "final_checklist": "final_checklist.md",
    "project_summary": "project_summary.md",
}

# ==============================================================================
# æ ¸å¿ƒä¸šåŠ¡ç¼–æ’é€»è¾‘
# ==============================================================================
# `event_generator` æ˜¯æœ¬æ¨¡å—çš„æ ¸å¿ƒï¼Œå®ƒå®šä¹‰äº†ä¸€ä¸ªåŒ…å«ä¸ƒå¤§é˜¶æ®µçš„æ ‡ä¹¦è§£ææµæ°´çº¿ï¼š
#
#   é˜¶æ®µ 1: æ–‡æ¡£é¢„å¤„ç† (Document Preprocessing)
#       - å°†è¾“å…¥çš„ .docx æ–‡ä»¶è½¬æ¢ä¸º Markdown æ ¼å¼ã€‚
#       - å¯¹ Markdown æ–‡æœ¬è¿›è¡Œç»“æ„åˆ†æå’Œæ™ºèƒ½åˆ†å—ã€‚
#
#   é˜¶æ®µ 2-5: å¹¶è¡Œ/ä¸²è¡Œåˆ†æ (Parallel/Serial Analysis)
#       - å¹¶è¡Œæˆ–ä¸²è¡Œåœ°è°ƒç”¨å››ä¸ªç‹¬ç«‹çš„ Agentï¼ˆå•†åŠ¡ã€æŠ€æœ¯ã€æŠ¥ä»·ã€è¯„åˆ†ï¼‰ã€‚
#       - æ¯ä¸ª Agent è´Ÿè´£ä»æ–‡æœ¬å—ä¸­æå–å…¶ä¸“ä¸šé¢†åŸŸçš„å†…å®¹ã€‚
#
#   é˜¶æ®µ 6: æ¨¡ç‰ˆæå– (Template Extraction)
#       - é‡‡ç”¨ä¸€ä¸ªå¤æ‚çš„â€œè¯†åˆ« -> åˆ†è¯Š -> æ”»åš -> æ±‡æ€»â€å››æ­¥æµç¨‹ã€‚
#       - å…ˆç”±â€œæ ‡å‡† Agentâ€å¿«é€Ÿè¯†åˆ«æ‰€æœ‰æ¨¡ç‰ˆï¼Œå†ç”±â€œéæ ‡ Agentâ€å¯¹ç–‘éš¾æ¨¡ç‰ˆè¿›è¡Œç²¾ç¡®æå–ã€‚
#
#   é˜¶æ®µ 7: æœ€ç»ˆæ¸…å•æ•´åˆ (Final Checklist Integration)
#       - æ•´ä¸ªæµæ°´çº¿çš„æ”¶å®˜ä¹‹ä½œï¼Œæ—¨åœ¨ç”Ÿæˆä¸€ä»½ä»¥â€œæ»¡åˆ†â€ä¸ºå¯¼å‘çš„è¡ŒåŠ¨æ¸…å•ã€‚
#       - å…ˆç”±â€œå¤§çº² Agentâ€æ ¹æ®è¯„åˆ†æ ‡å‡†ï¼Œæ„å»ºå‡ºæ¸…å•çš„éª¨æ¶ã€‚
#       - å†ç”±â€œå¯ŒåŒ– Agentâ€åˆ†ä¸‰æ¬¡ï¼Œå°†å•†åŠ¡ã€æŠ€æœ¯ã€æŠ¥ä»·çš„ç»†èŠ‚å¡«å……è¿›å»ã€‚
#
# æ•´ä¸ªè¿‡ç¨‹é€šè¿‡ Server-Sent Events (SSE) åè®®ï¼Œå®æ—¶åœ°å°†è¿›åº¦ã€æ—¥å¿—ã€äº§ç‰©ç­‰
# äº‹ä»¶æ¨é€ç»™å‰ç«¯ï¼Œå®ç°äº†é«˜åº¦çš„é€æ˜åº¦å’Œå®æ—¶åé¦ˆã€‚
# ==============================================================================


# ----------------- æ–‡æ¡£å¤„ç†é€»è¾‘ (å‡çº§åˆ†å—èƒ½åŠ› + æœ¬åœ° Docx è§£æ) -----------------

# --- æ ¸å¿ƒæ”¹åŠ¨ï¼šä» document_processor.py æ¬è¿å¹¶æ•´åˆ Docx è§£æé€»è¾‘ ---
def _table_to_markdown(table: docx.table.Table) -> str:
    """å°† docx.table.Table å¯¹è±¡è½¬æ¢ä¸º Markdown æ ¼å¼çš„å­—ç¬¦ä¸²ã€‚"""
    md_table = []
    for i, row in enumerate(table.rows):
        cell_texts = [" ".join(cell.text.split()).strip() for cell in row.cells]
        md_table.append("| " + " | ".join(cell_texts) + " |")
        if i == 0:
            separator = ["---" for _ in row.cells]
            md_table.append("| " + " | ".join(separator) + " |")
    return "\n".join(md_table)

def convert_docx_to_markdown(docx_path: str) -> str:
    """
    å°† DOCX æ–‡ä»¶è½¬æ¢ä¸ºå•ä¸ª Markdown æ–‡æœ¬æµï¼Œèƒ½å¤Ÿæ­£ç¡®å¤„ç†æ®µè½å’Œè¡¨æ ¼ã€‚
    """
    document = docx.Document(docx_path)
    text_lines = []

    for element in document.element.body:
        if isinstance(element, CT_P):
            para = docx.text.paragraph.Paragraph(element, document)
            text_lines.append(para.text)
        elif isinstance(element, CT_Tbl):
            table = docx.table.Table(element, document)
            md_table_str = _table_to_markdown(table)
            text_lines.append("\n" + md_table_str + "\n")

    return "\n".join(text_lines)


def get_token_count(text: str) -> int:
    """ä½¿ç”¨ tiktoken è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡ã€‚"""
    encoder = tiktoken.get_encoding("cl100k_base")
    return len(encoder.encode(text))

def analyze_structure(text: str) -> List[Dict[str, Any]]:
    # (å†…å®¹ä¸ä¹‹å‰ä¸€è‡´, ä¸ºäº†ç®€æ´æ€§åœ¨æ­¤æŠ˜å )
    lines = text.split('\n')
    title_pattern = re.compile(r"^\s*#*\s*ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ç« \s+[^\t\.]*$")
    found_titles = [{"title": line.strip('# ').strip(), "line_num": i} for i, line in enumerate(lines) if title_pattern.match(line.strip())]
    structure = []
    for i, title_info in enumerate(found_titles):
        start_line = title_info["line_num"]
        end_line = found_titles[i+1]["line_num"] if i + 1 < len(found_titles) else len(lines)
        content_preview = "".join(lines[start_line+1:end_line]).strip()
        if content_preview:
            structure.append({"title": title_info["title"], "text": "\n".join(lines[start_line:end_line]).strip()})
    if not structure: return [{"title": "å®Œæ•´æ–‡æ¡£", "text": text}]
    return structure

def chunk_content(sections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    å°†ç« èŠ‚æ–‡æœ¬åˆ†å‰²æˆé€‚åˆæ¨¡å‹å¤„ç†çš„å—ã€‚
    ç°åœ¨æ”¯æŒåŸºäº Token çš„äºŒæ¬¡åˆ†å—ï¼Œå¹¶èƒ½å¤„ç†è¶…é•¿å—ã€‚
    """
    final_chunks = []
    OVERLAP_PARA_COUNT = 2 # å®šä¹‰é‡å çš„æ®µè½æ•°é‡

    for section in sections:
        section_text = section['text']
        token_count = get_token_count(section_text)

        if token_count <= MAX_TOKENS_PER_CHUNK:
            # å—å¤§å°åˆé€‚ï¼Œç›´æ¥æ·»åŠ 
            final_chunks.append({
                "source_title": section['title'],
                "content": section_text
            })
        else:
            # å—å¤ªå¤§ï¼Œéœ€è¦æŒ‰æ®µè½è¿›è¡Œå­åˆ†å—
            print(f"   - æ£€æµ‹åˆ°è¶…é•¿å— (æ ‡é¢˜: '{section['title']}', Tokens: {token_count})ï¼Œæ­£åœ¨è¿›è¡Œå­åˆ†å—...")
            
            sub_chunks_text = []
            current_sub_chunk_paras = []
            current_token_count = 0
            
            paragraphs = [p for p in section_text.split('\n') if p.strip()]

            for para in paragraphs:
                para_token_count = get_token_count(para) + 1 # +1 for newline token
                
                if current_token_count + para_token_count > MAX_TOKENS_PER_CHUNK and current_sub_chunk_paras:
                    sub_chunks_text.append("\n".join(current_sub_chunk_paras))
                    overlap_paras = current_sub_chunk_paras[-OVERLAP_PARA_COUNT:]
                    current_sub_chunk_paras = overlap_paras
                    current_token_count = get_token_count("\n".join(current_sub_chunk_paras))

                current_sub_chunk_paras.append(para)
                current_token_count += para_token_count
            
            if current_sub_chunk_paras:
                sub_chunks_text.append("\n".join(current_sub_chunk_paras))

            # å°†å­å—åˆ—è¡¨è½¬æ¢ä¸ºæœ€ç»ˆçš„ chunk å­—å…¸
            for sub_chunk_text in sub_chunks_text:
                final_chunks.append({
                    "source_title": section['title'],
                    "content": sub_chunk_text
                })
    
    return final_chunks

# --- æ ¸å¿ƒæ”¹åŠ¨ï¼šåˆ›å»ºä¸€ä¸ªæ–°çš„â€œåŒ…è£…å™¨â€åç¨‹ ---
async def run_phase_and_collect_artifacts(phase_stream):
    """
    ä¸€ä¸ªä¸“é—¨ä¸ºå¹¶è¡Œæ‰§è¡Œæ¨¡å¼è®¾è®¡çš„â€œå¼‚æ­¥ç”Ÿæˆå™¨æ¶ˆè´¹å™¨â€ã€‚

    `asyncio.gather` æ— æ³•ç›´æ¥å¤„ç†å¼‚æ­¥ç”Ÿæˆå™¨ã€‚æ­¤å‡½æ•°çš„ä½œç”¨æ˜¯ï¼Œ
    å®Œæ•´åœ°ã€é™é»˜åœ°éå†å®Œä¸€ä¸ªåˆ†æé˜¶æ®µï¼ˆ`run_extraction_phase`ï¼‰çš„æ‰€æœ‰äº‹ä»¶ï¼Œ
    ç„¶ååªæ”¶é›†å¹¶è¿”å›æœ€ç»ˆçš„â€œäº§ç‰©â€(artifact) äº‹ä»¶ã€‚

    è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥åœ¨åå°å¹¶è¡Œè¿è¡Œå¤šä¸ªåˆ†æé˜¶æ®µï¼Œå¹¶åœ¨æœ€åç»Ÿä¸€è·å–å®ƒä»¬çš„äº§å‡ºã€‚

    Args:
        phase_stream: ä¸€ä¸ª `run_extraction_phase` å‡½æ•°è¿”å›çš„å¼‚æ­¥ç”Ÿæˆå™¨ã€‚

    Returns:
        List[str]: ä¸€ä¸ªåªåŒ…å«æœ€ç»ˆäº§ç‰©äº‹ä»¶çš„åˆ—è¡¨ã€‚
    """
    final_events = []
    async for event in phase_stream:
        # æˆ‘ä»¬å¿…é¡»éå†æ•´ä¸ªæµï¼Œä»¥ç¡®ä¿ run_extraction_phase çš„ä»£ç è¢«å®Œæ•´æ‰§è¡Œ
        if event.startswith("event: artifact"):
            final_events.append(event)
    return final_events


# ----------------- æ ¸å¿ƒä¸šåŠ¡ç¼–æ’é€»è¾‘ (é‡æ„æˆå¤šé˜¶æ®µ) -----------------

def sse(event: str, data_obj: Dict) -> str:
    """
    å°†äº‹ä»¶ç±»å‹å’Œæ•°æ®å¯¹è±¡ï¼Œæ ¼å¼åŒ–ä¸ºç¬¦åˆ Server-Sent Events (SSE) è§„èŒƒçš„å­—ç¬¦ä¸²ã€‚

    Args:
        event (str): äº‹ä»¶çš„ç±»å‹ (e.g., "log", "artifact", "phase_start")ã€‚
        data_obj (Dict): è¦å‘é€çš„æ•°æ®ï¼Œå°†è¢«åºåˆ—åŒ–ä¸º JSON å­—ç¬¦ä¸²ã€‚

    Returns:
        str: ä¸€ä¸ªå¯ä»¥ç›´æ¥å‘é€ç»™å®¢æˆ·ç«¯çš„ SSE æ ¼å¼çš„æ–‡æœ¬å—ã€‚
    """
    return f"event: {event}\n" + f"data: {json.dumps(data_obj, ensure_ascii=False, default=str)}\n\n"

async def mcp_smart_write(mcp_client: MCPClient, file_path: str, content: str) -> bool:
    """
    é€šè¿‡ MCP æœåŠ¡ï¼Œä»¥ä¸€ç§å¥å£®çš„æ–¹å¼å†™å…¥æˆ–è¦†ç›–æ–‡ä»¶å†…å®¹ã€‚

    å®ƒä¼šè‡ªåŠ¨å¤„ç†æ–‡ä»¶æ˜¯å¦å­˜åœ¨çš„æƒ…å†µï¼Œç¡®ä¿å†…å®¹èƒ½è¢«æ­£ç¡®å†™å…¥ã€‚

    Args:
        mcp_client (MCPClient): å·²åˆå§‹åŒ–çš„ MCP å®¢æˆ·ç«¯å®ä¾‹ã€‚
        file_path (str): ç›®æ ‡æ–‡ä»¶çš„è·¯å¾„ï¼ˆç›¸å¯¹äº MCP æœåŠ¡çš„å·¥ä½œç›®å½•ï¼‰ã€‚
        content (str): è¦å†™å…¥çš„å®Œæ•´æ–‡ä»¶å†…å®¹ã€‚

    Returns:
        bool: å†™å…¥æ“ä½œæ˜¯å¦æˆåŠŸã€‚
    """
    try:
        old_text = ""
        file_exists = False
        try:
            read_res = await mcp_client.call_tool("read_file", {"path": file_path, "limit": 10000000})
            text_from_read = str(getattr(read_res, "data", read_res))
            if "æ–‡ä»¶æœªæ‰¾åˆ°" not in text_from_read and "file not found" not in text_from_read.lower():
                file_exists = True
                old_text = text_from_read
        except Exception:
            file_exists = False

        if not file_exists or old_text == "":
            # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šä½¿ç”¨ smart_edit æ¥å®ç° write_file çš„åŠŸèƒ½ ---
            await mcp_client.call_tool("smart_edit", {
                "file_path": file_path, 
                "old_string": "", 
                "new_string": content
            })
        else:
            await mcp_client.call_tool("smart_edit", {
                "file_path": file_path, 
                "old_string": old_text, 
                "new_string": content
            })
        
        # å†™å…¥åæ ¡éªŒ
        for i in range(3): # ç®€åŒ–æ ¡éªŒæ¬¡æ•°
            await asyncio.sleep(0.1 * (i + 1))
            val_res = await mcp_client.call_tool("read_file", {"path": file_path, "limit": 1})
            if "æ–‡ä»¶æœªæ‰¾åˆ°" not in str(getattr(val_res, "data", val_res)):
                return True
        return False
    except Exception:
        return False

async def run_extraction_phase(
    phase_name: str,
    phase_key: str, # <-- æ ¸å¿ƒæ”¹åŠ¨ï¼šå¢åŠ ä¸€ä¸ªä¸“é—¨ç”¨äºæŸ¥æ‰¾çš„ key
    agent_factory: Callable[..., Agent],
    text_chunks_with_meta: List[Dict[str, Any]],
    run_config: RunConfig,
    mcp_client: MCPClient,
    language: str = "zh",
):
    """
    ä¸€ä¸ªé€šç”¨çš„è¾…åŠ©å‡½æ•°ï¼Œç”¨äºæ‰§è¡Œå•ä¸ªæå–é˜¶æ®µï¼ˆä¾‹å¦‚å•†åŠ¡ã€æŠ€æœ¯ç­‰ï¼‰ã€‚
    å®ƒä¼šéå†æ–‡æœ¬å—ï¼Œæµå¼è°ƒç”¨ Agentï¼Œå¹¶å®æ—¶æ¨é€äº‹ä»¶ï¼Œæœ€ç»ˆå°†ç»“æœå†™å…¥æ–‡ä»¶ã€‚
    """
    yield sse("phase_start", {"name": phase_name})
    
    agent = agent_factory(language=language)
    full_extracted_content = ""
    
    for i, chunk_info in enumerate(text_chunks_with_meta):
        chunk_text = chunk_info['content']
        yield sse("update", {"phase": phase_name, "progress": f"{i+1}/{len(text_chunks_with_meta)}"})
        
        yield sse("stream_start", {"chunk": i + 1, "phase": phase_name})
        
        result_stream = Runner.run_streamed(
            agent, 
            f"è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå– {phase_name}ï¼š\n\n---\n\n{chunk_text}", 
            run_config=run_config
        )
        
        extracted_content_chunk = ""
        async for event in result_stream.stream_events():
            if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
                delta = event.data.delta
                extracted_content_chunk += delta
                yield sse("token_delta", {"delta": delta})

        yield sse("stream_end", {"chunk": i + 1, "phase": phase_name})
        
        if extracted_content_chunk and "æœªæ‰¾åˆ°" not in extracted_content_chunk:
            source_title = chunk_info['source_title']
            new_section = f"## æ¥è‡ªç« èŠ‚: {source_title}\n\n{extracted_content_chunk}\n\n---\n\n"
            full_extracted_content += new_section
            yield sse("note", {"phase": phase_name, "text": f"å— {i+1} åˆ†æå®Œæˆï¼Œæå–åˆ°å†…å®¹ã€‚"})
        else:
            yield sse("note", {"phase": phase_name, "text": f"å— {i+1} åˆ†æå®Œæˆï¼Œæœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"})
    
    # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šä½¿ç”¨æ–°çš„ phase_key æ¥æŸ¥æ‰¾æ–‡ä»¶å ---
    output_filename = OUTPUT_PATHS[phase_key]
    doc_title = os.path.basename(DEFAULT_DOCX_PATH) #
    final_md_content = f"# {doc_title} - {phase_name}åˆ†ææŠ¥å‘Š\n\n" + full_extracted_content
    await mcp_smart_write(mcp_client, output_filename, final_md_content)
    
    yield sse("artifact", {"type": "file", "filename": output_filename})
    yield sse("phase_end", {"name": phase_name})


# ----------------- è¾…åŠ©å‡½æ•°åŒº (æ–°å¢æ¸…å•æ•´åˆè¾…åŠ©å‡½æ•°) -----------------

def split_outline_by_headings(outline_text: str) -> Dict[str, str]:
    """
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œå°†ä¸€ä»½å®Œæ•´çš„ Markdown å¤§çº²æ–‡æœ¬ï¼ŒæŒ‰ä¸€çº§æ ‡é¢˜ï¼ˆ# headingï¼‰
    æ‹†åˆ†ä¸ºä¸€ä¸ªå­—å…¸ã€‚è¿™ä½¿å¾—åç»­å¯ä»¥å¯¹å¤§çº²çš„å„ä¸ªéƒ¨åˆ†è¿›è¡Œç‹¬ç«‹å¤„ç†ã€‚

    Args:
        outline_text (str): åŒ…å« Markdown æ ‡é¢˜çš„å®Œæ•´æ–‡æœ¬ã€‚

    Returns:
        Dict[str, str]: ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯ä»æ ‡é¢˜ä¸­è¯†åˆ«å‡ºçš„æ ¸å¿ƒè¯ï¼ˆ"business", 
                        "technical", "pricing"ï¼‰ï¼Œå€¼æ˜¯åŒ…å«æ ‡é¢˜åœ¨å†…çš„å®Œæ•´éƒ¨åˆ†æ–‡æœ¬ã€‚
    """
    sections = {}
    parts = re.split(r'(^#\s.*)', outline_text, flags=re.MULTILINE)
    
    if not parts:
        return {"": outline_text}

    for i in range(1, len(parts), 2):
        title = parts[i].strip()
        content = parts[i+1].strip() if i + 1 < len(parts) else ""
        
        if "å•†åŠ¡" in title:
            key = "business"
        elif "æŠ€æœ¯" in title:
            key = "technical"
        elif "ä»·æ ¼" in title:
            key = "pricing"
        else:
            key = title
            
        sections[key] = f"{title}\n\n{content}".strip()
        
    return sections

async def mcp_read_file(mcp_client: MCPClient, file_path: str) -> str | None:
    """
    é€šè¿‡ MCP æœåŠ¡ï¼Œä»¥ä¸€ç§å¥å£®çš„æ–¹å¼è¯»å–æ–‡ä»¶å†…å®¹ã€‚

    Args:
        mcp_client (MCPClient): å·²åˆå§‹åŒ–çš„ MCP å®¢æˆ·ç«¯å®ä¾‹ã€‚
        file_path (str): ç›®æ ‡æ–‡ä»¶çš„è·¯å¾„ï¼ˆç›¸å¯¹äº MCP æœåŠ¡çš„å·¥ä½œç›®å½•ï¼‰ã€‚

    Returns:
        str | None: å¦‚æœæˆåŠŸï¼Œè¿”å›æ–‡ä»¶å†…å®¹ï¼›å¦‚æœæ–‡ä»¶æœªæ‰¾åˆ°æˆ–å‘ç”Ÿé”™è¯¯ï¼Œè¿”å› Noneã€‚
    """
    try:
        read_res = await mcp_client.call_tool("read_file", {"path": file_path, "limit": 10000000})
        content = str(getattr(read_res, "data", read_res))
        if "æ–‡ä»¶æœªæ‰¾åˆ°" in content or "file not found" in content.lower():
            return None
        return content
    except Exception:
        return None

async def event_generator(
    docx_path: str = DEFAULT_DOCX_PATH,
    model_name: str = DEFAULT_MODEL_NAME,
    language: str = "zh",
    stream_token_deltas: bool = True # <-- æ ¸å¿ƒæ”¹åŠ¨ï¼šæ¥æ”¶é…ç½®å‚æ•°
):
    """
    æ ‡ä¹¦è§£ææµæ°´çº¿çš„æ€»å…¥å£å’Œæ ¸å¿ƒäº‹ä»¶ç”Ÿæˆå™¨ã€‚

    å®ƒæŒ‰ç…§é¢„è®¾çš„ä¸ƒå¤§é˜¶æ®µï¼Œä¾æ¬¡æˆ–å¹¶è¡Œåœ°æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼Œå¹¶é€šè¿‡ `yield` è¯­å¥ï¼Œ
    ä»¥ Server-Sent Events (SSE) çš„å½¢å¼ï¼Œå‘å¤–å®æ—¶æ¨é€æ•´ä¸ªè¿‡ç¨‹çš„çŠ¶æ€ã€‚

    Args:
        docx_path (str): è¦è§£æçš„ .docx æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„ã€‚
        model_name (str): ç”¨äºæ‰§è¡Œä»»åŠ¡çš„ LLM æ¨¡å‹åç§°ã€‚
        language (str): Agent è¾“å‡ºå†…å®¹çš„è¯­è¨€ã€‚
        stream_token_deltas (bool): æ§åˆ¶åˆ†æé˜¶æ®µï¼ˆ2-5ï¼‰çš„æ‰§è¡Œæ¨¡å¼ã€‚
                                    True  -> ä¸²è¡Œæ‰§è¡Œï¼Œå¹¶å®æ—¶æ¨é€æ‰€æœ‰ token æµã€‚
                                    False -> å¹¶è¡Œæ‰§è¡Œï¼Œåªå®æ—¶æ¨é€ä¸€ä¸ªéšæœºä»»åŠ¡çš„ token æµã€‚

    Yields:
        str: æ ¼å¼åŒ–åçš„ SSE äº‹ä»¶å­—ç¬¦ä¸²ã€‚
    """
    # 1. åˆå§‹åŒ–
    litellm_model = LitellmModel(model=model_name, api_key=settings.OPENAI_API_KEY)
    run_config = RunConfig(
        model=litellm_model, model_settings=ModelSettings(include_usage=False), tracing_disabled=True
    )
    mcp_client = MCPClient(settings.MCP_SERVER_URL)
    
    try:
        # --- é˜¶æ®µ 1: æ–‡æ¡£é¢„å¤„ç† ---
        yield sse("phase_start", {"name": "æ–‡æ¡£é¢„å¤„ç†"})
        yield sse("note", {"phase": "æ–‡æ¡£é¢„å¤„ç†", "text": f"æ­£åœ¨è¯»å–å¹¶è½¬æ¢æ–‡æ¡£: {os.path.basename(docx_path)}..."})
        
        # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šæ›¿æ¢ MCP è°ƒç”¨ä¸ºæœ¬åœ°å‡½æ•°è°ƒç”¨ ---
        try:
            markdown_content = convert_docx_to_markdown(docx_path)
            yield sse("note", {"phase": "æ–‡æ¡£é¢„å¤„ç†", "text": "æ–‡æ¡£è½¬æ¢æˆåŠŸï¼"})
        except Exception as e:
            yield sse("error", {"type": "DocxConversionError", "message": f"å¤„ç† Docx æ–‡ä»¶æ—¶å‡ºé”™: {e}"})
            return

        # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šä¿å­˜å¹¶æ¨é€ Markdown å…¨æ–‡ ---
        async with mcp_client:
            await mcp_smart_write(mcp_client, OUTPUT_PATHS["intermediate_md"], markdown_content)
        yield sse("artifact", {"type": "file", "filename": OUTPUT_PATHS["intermediate_md"]})

        # --- æ–°é˜¶æ®µï¼šé¡¹ç›®æ¦‚è¦ç”Ÿæˆ ---
        yield sse("phase_start", {"name": "é¡¹ç›®æ¦‚è¦ç”Ÿæˆ"})
        yield sse("note", {"phase": "é¡¹ç›®æ¦‚è¦ç”Ÿæˆ", "text": "æ­£åœ¨è°ƒç”¨ LLM ç”Ÿæˆé¡¹ç›®æ¦‚è¦æè¿°..."})
        summary_agent = project_summary_agent(language=language)
        summary_input_text = markdown_content
        truncated = False
        if len(summary_input_text) > SUMMARY_INPUT_CHAR_LIMIT:
            summary_input_text = summary_input_text[:SUMMARY_INPUT_CHAR_LIMIT]
            truncated = True
        summary_prompt = (
            "è¯·æ ¹æ®ä»¥ä¸‹æ‹›æ ‡æ–‡ä»¶ï¼ˆMarkdown å½¢å¼ï¼‰çš„å†…å®¹ï¼Œè¾“å‡ºç¬¦åˆæŒ‡ä»¤è¦æ±‚çš„é¡¹ç›®æ¦‚è¦ï¼š\n\n"
            f"{summary_input_text}"
        )
        summary_result = await Runner.run(summary_agent, summary_prompt, run_config=run_config)
        summary_text = summary_result.final_output.strip()
        if not summary_text:
            summary_text = "ï¼ˆæ¨¡å‹æœªç”Ÿæˆæœ‰æ•ˆå†…å®¹ï¼‰"
        metadata_notice = (
            "" if truncated else ""
        )
        project_summary_md = f"# æ‹›æ ‡é¡¹ç›®æ¦‚è¦\n\n{summary_text}{metadata_notice}\n"
        async with mcp_client:
            await mcp_smart_write(
                mcp_client,
                OUTPUT_PATHS["project_summary"],
                project_summary_md,
            )
        yield sse("artifact", {"type": "file", "filename": OUTPUT_PATHS["project_summary"]})
        yield sse("phase_end", {"name": "é¡¹ç›®æ¦‚è¦ç”Ÿæˆ"})


        yield sse("note", {"phase": "æ–‡æ¡£é¢„å¤„ç†", "text": "æ­£åœ¨åˆ†ææ–‡æ¡£ç»“æ„å¹¶è¿›è¡Œæ–‡æœ¬åˆ†å—..."})
        sections = analyze_structure(markdown_content)
        final_chunks_with_meta = chunk_content(sections)
        text_chunks = [chunk['content'] for chunk in final_chunks_with_meta] # æå–çº¯æ–‡æœ¬å†…å®¹åˆ—è¡¨
        yield sse("note", {"phase": "æ–‡æ¡£é¢„å¤„ç†", "text": f"æ–‡æ¡£åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(text_chunks)} ä¸ªæ–‡æœ¬å—ã€‚"})

        # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šä¿å­˜å¹¶æ¨é€åˆ†å—ç»“æœ ---
        chunks_json_content = json.dumps(
            final_chunks_with_meta,
            ensure_ascii=False,
            indent=2
        )
        async with mcp_client:
            await mcp_smart_write(mcp_client, OUTPUT_PATHS["intermediate_chunks"], chunks_json_content)
        yield sse("artifact", {"type": "file", "filename": OUTPUT_PATHS["intermediate_chunks"]})
        yield sse("phase_end", {"name": "æ–‡æ¡£é¢„å¤„ç†"})

        # --- é˜¶æ®µ 2-5: é¡ºåºæ‰§è¡Œå„é¡¹æå– ---
        # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šæ ¹æ® stream_token_deltas çš„å€¼ï¼Œé€‰æ‹©æ‰§è¡Œæ¨¡å¼ ---
        if stream_token_deltas:
            # --- æ¨¡å¼ä¸€ï¼šä¸²è¡Œæ‰§è¡Œï¼Œå®æ—¶æ¨é€ Token æµ (ä¾¿äºè°ƒè¯•) ---
            yield sse("note", {"phase": "åˆ†æ", "text": "ä»¥ä¸²è¡Œæ¨¡å¼å¯åŠ¨åˆ†æï¼Œå°†å®æ—¶æ¨é€ Token æµ..."})
            async with mcp_client:
                async for event in run_extraction_phase("å•†åŠ¡è¦æ±‚", "business", business_requirement_extractor_agent, final_chunks_with_meta, run_config, mcp_client, language): yield event
                async for event in run_extraction_phase("æŠ€æœ¯è¦æ±‚", "technical", technical_requirement_extractor_agent, final_chunks_with_meta, run_config, mcp_client, language): yield event
                async for event in run_extraction_phase("æŠ¥ä»·è¦æ±‚", "pricing", pricing_requirement_extractor_agent, final_chunks_with_meta, run_config, mcp_client, language): yield event
                async for event in run_extraction_phase("è¯„åˆ†è¦æ±‚", "scoring", scoring_requirement_extractor_agent, final_chunks_with_meta, run_config, mcp_client, language): yield event
        else:
            # --- æ¨¡å¼ä¸‰ï¼šå¹¶è¡Œæ‰§è¡Œï¼Œä½†å®æ—¶æ¨é€ä¸€ä¸ªä»»åŠ¡çš„æµ (å…¼é¡¾æ•ˆç‡ä¸åé¦ˆ) ---
            yield sse("phase_start", {"name": "å¹¶è¡Œåˆ†æ"})
            yield sse("note", {"phase": "å¹¶è¡Œåˆ†æ", "text": "ä»¥å¹¶è¡Œæ¨¡å¼å¯åŠ¨åˆ†æï¼Œå°†å®æ—¶æ¨é€å…¶ä¸­ä¸€ä¸ªä»»åŠ¡çš„è¿›åº¦æµ..."})

            async with mcp_client:
                all_phases = {
                    "business": ("å•†åŠ¡è¦æ±‚", business_requirement_extractor_agent),
                    "technical": ("æŠ€æœ¯è¦æ±‚", technical_requirement_extractor_agent),
                    "pricing": ("æŠ¥ä»·è¦æ±‚", pricing_requirement_extractor_agent),
                    "scoring": ("è¯„åˆ†è¦æ±‚", scoring_requirement_extractor_agent),
                }

                if not all_phases:
                    yield sse("phase_end", {"name": "å¹¶è¡Œåˆ†æ"})
                else:
                    # éšæœºé€‰æ‹©ä¸€ä¸ª phase è¿›è¡Œæµå¼æ¨é€
                    stream_phase_key = random.choice(list(all_phases.keys()))
                    stream_phase_name, stream_agent_factory = all_phases.pop(stream_phase_key)
                    yield sse("note", {"phase": "å¹¶è¡Œåˆ†æ", "text": f"å·²éšæœºé€‰æ‹© '{stream_phase_name}' ä»»åŠ¡è¿›è¡Œå®æ—¶æµæ¨é€ã€‚"})

                    # å°†å…¶ä»– phase ä½œä¸ºåå°ä»»åŠ¡è¿è¡Œ
                    background_tasks = []
                    for key, (name, factory) in all_phases.items():
                        task_coro = run_phase_and_collect_artifacts(
                            run_extraction_phase(name, key, factory, final_chunks_with_meta, run_config, mcp_client, language)
                        )
                        background_tasks.append(asyncio.create_task(task_coro))

                    # è¿è¡Œå¹¶æ¨é€ä¸»ä»»åŠ¡çš„æµ
                    stream_generator = run_extraction_phase(
                        stream_phase_name, stream_phase_key, stream_agent_factory, final_chunks_with_meta, run_config, mcp_client, language
                    )
                    async for event in stream_generator:
                        yield event

                    # ç­‰å¾…åå°ä»»åŠ¡å®Œæˆå¹¶æ¨é€å®ƒä»¬çš„æœ€ç»ˆäº§ç‰©
                    if background_tasks:
                        yield sse("note", {"phase": "å¹¶è¡Œåˆ†æ", "text": "æ­£åœ¨ç­‰å¾…å…¶ä½™åå°ä»»åŠ¡å®Œæˆ..."})
                        results_of_events = await asyncio.gather(*background_tasks)
                        for event_list in results_of_events:
                            for event in event_list:
                                yield event
                        yield sse("note", {"phase": "å¹¶è¡Œåˆ†æ", "text": "æ‰€æœ‰åå°ä»»åŠ¡å‡å·²å®Œæˆã€‚"})

            yield sse("phase_end", {"name": "å¹¶è¡Œåˆ†æ"})

        # --- é˜¶æ®µ 6: æ¨¡ç‰ˆæå– (v4.0 ç»ˆæç‰ˆæµæ°´çº¿) ---
        yield sse("phase_start", {"name": "æ¨¡ç‰ˆæå–"})
        
        # --- æ­¥éª¤ A: è¯†åˆ«ä¸æ‰“æ ‡ ---
        yield sse("note", {"phase": "æ¨¡ç‰ˆæå–", "text": "æ­¥éª¤ A: æ­£åœ¨è¿›è¡Œåˆæ­¥è¯†åˆ«ä¸æ‰“æ ‡..."})
        standard_agent = standard_template_extractor_agent(language=language)
        all_found_templates = []
        async with mcp_client:
            for i, chunk_info in enumerate(final_chunks_with_meta):
                result = await Runner.run(standard_agent, 
                    f"è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–æ¨¡ç‰ˆï¼š\n\n---\n\n{chunk_info['content']}", 
                    run_config=run_config
                )
                try:
                    json_str = result.final_output.strip().replace("`", "")
                    if json_str.startswith("json"): json_str = json_str[4:]
                    
                    templates_from_chunk = json.loads(json_str)

                    if isinstance(templates_from_chunk, list):
                        for template in templates_from_chunk:
                            template["source_chunk_ids"] = [i]
                        all_found_templates.extend(templates_from_chunk)
                except (json.JSONDecodeError, AttributeError, TypeError):
                    pass # Silently ignore parsing errors in production
        
        # --- æ­¥éª¤ B: åˆ†è¯Š ---
        yield sse("note", {"phase": "æ¨¡ç‰ˆæå–", "text": "æ­¥éª¤ B: æ­£åœ¨å¯¹æ¨¡æ¿è¿›è¡Œåˆ†è¯Š..."})
        standard_results = [tpl for tpl in all_found_templates if tpl.get('key') is not None]
        non_standard_to_process = [tpl for tpl in all_found_templates if tpl.get('key') is None]
        yield sse("note", {"phase": "æ¨¡ç‰ˆæå–", "text": f"åˆ†è¯Šå®Œæˆï¼š{len(standard_results)} ä¸ªæ ‡å‡†æ¨¡æ¿ï¼Œ{len(non_standard_to_process)} ä¸ªå¾…å¤„ç†éæ ‡æ¨¡æ¿ã€‚"})

        # --- æ­¥éª¤ C: ä¸“å®¶ä¼šè¯Š (éæ ‡æå–) ---
        yield sse("note", {"phase": "æ¨¡ç‰ˆæå–", "text": "æ­¥éª¤ C: æ­£åœ¨è¿›è¡Œéæ ‡æå–ï¼ˆæ”»åšï¼‰..."})
        non_standard_agent = non_standard_template_extractor_agent(language=language)
        non_standard_results = []
        # ä¸ºäº†æ•ˆç‡ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰éæ ‡æ¨¡æ¿æŒ‰å…¶æ¥æºæ–‡æœ¬å—è¿›è¡Œåˆ†ç»„
        grouped_to_process = {}
        for tpl in non_standard_to_process:
            # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šä½¿ç”¨æ–°çš„ã€æ­£ç¡®çš„ `source_chunk_ids` å­—æ®µ ---
            if tpl.get("source_chunk_ids"):
                chunk_idx = tpl['source_chunk_ids'][0]
                if chunk_idx not in grouped_to_process:
                    grouped_to_process[chunk_idx] = []
                grouped_to_process[chunk_idx].append(tpl) # ä¼ é€’å®Œæ•´çš„ tpl å¯¹è±¡

        async with mcp_client:
            for chunk_idx, templates_in_chunk in grouped_to_process.items():
                chunk_text = final_chunks_with_meta[chunk_idx]['content']
                # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šä» tpl å¯¹è±¡ä¸­æå– name ---
                names_to_extract = [t['name'] for t in templates_in_chunk]
                names_str = ", ".join(f"'{n}'" for n in names_to_extract)
                
                result = await Runner.run(
                    non_standard_agent,
                    f"å¾…æå–çš„æ¨¡æ¿åç§°åˆ—è¡¨: [{names_str}]\n\n---\n\næ‹›æ ‡æ–‡ä»¶æ–‡æœ¬:\n{chunk_text}",
                    run_config=run_config
                )
                try:
                    json_str = result.final_output.strip().replace("`", "")
                    if json_str.startswith("json"): json_str = json_str[4:]
                    
                    templates_from_chunk = json.loads(json_str)

                    if isinstance(templates_from_chunk, list):
                        non_standard_results.extend(templates_from_chunk)
                except (json.JSONDecodeError, AttributeError, TypeError):
                    pass # Silently ignore
        yield sse("note", {"phase": "æ¨¡ç‰ˆæå–", "text": f"æ­¥éª¤ C å®Œæˆï¼šæˆåŠŸæå– {len(non_standard_results)} ä¸ªéæ ‡æ¨¡æ¿ã€‚"})
        
        # --- æ­¥éª¤ D: ç»“æœæ±‡æ€»ä¸æœ€ç»ˆå¤„ç† ---
        yield sse("note", {"phase": "æ¨¡ç‰ˆæå–", "text": "æ­¥éª¤ D: æ­£åœ¨åˆå¹¶ã€å»é‡å¹¶æ ¼å¼åŒ–..."})
        
        final_results = []
        processed_names = set()

        # é¦–å…ˆå¤„ç†æ ‡å‡†æ¨¡æ¿ç»“æœï¼Œåº”ç”¨ä¸¥æ ¼çš„å­—æ®µæ§åˆ¶
        for std_tpl in standard_results:
            name = std_tpl.get("name", "").strip()
            if name and name not in processed_names:
                final_results.append({
                    "id": f"tpl_{uuid.uuid4().hex[:8]}",
                    "name": name,
                    "title": std_tpl.get("key") 
                })
                processed_names.add(name)

        # ç„¶åå¤„ç†éæ ‡æ¨¡æ¿ç»“æœï¼Œåº”ç”¨ä¸¥æ ¼çš„å­—æ®µæ§åˆ¶å’Œå®‰å…¨çš„ .get() è®¿é—®
        for ns_tpl in non_standard_results:
            name = ns_tpl.get("name", "").strip()
            if name and name not in processed_names:
                # ä¸¥æ ¼çš„å­—æ®µæ§åˆ¶ï¼Œå¹¶ä½¿ç”¨ .get() æ–¹æ³•ç¡®ä¿å®‰å…¨
                final_results.append({
                    "id": f"tpl_{uuid.uuid4().hex[:8]}",
                    "name": name,
                    "start": ns_tpl.get("start"),
                    "end": ns_tpl.get("end"),
                    "keywords": ns_tpl.get("keywords", [])
                })
                processed_names.add(name)

        yield sse("note", {"phase": "æ¨¡ç‰ˆæå–", "text": f"æ­¥éª¤ D å®Œæˆï¼šå…±ç”Ÿæˆ {len(final_results)} ä¸ªæœ€ç»ˆæ¨¡æ¿è®°å½•ã€‚"})
        
        # å†™å…¥æœ€ç»ˆçš„ JSON æ–‡ä»¶
        final_json_content = json.dumps(final_results, ensure_ascii=False, indent=4)
        async with mcp_client:
            await mcp_smart_write(mcp_client, OUTPUT_PATHS["template"], final_json_content)
        
        yield sse("artifact", {"type": "file", "filename": OUTPUT_PATHS["template"]})
        yield sse("phase_end", {"name": "æ¨¡ç‰ˆæå–"})

        # --- é˜¶æ®µ 7: æœ€ç»ˆæ¸…å•æ•´åˆ (ä» test_checklist_pipeline.py ç§»æ¤) ---
        yield sse("phase_start", {"name": "æœ€ç»ˆæ¸…å•æ•´åˆ"})

        # --- æ­¥éª¤ 7.1: è¯»å–æ‰€æœ‰åˆ†ææŠ¥å‘Š ---
        yield sse("note", {"phase": "æœ€ç»ˆæ¸…å•æ•´åˆ", "text": "æ­¥éª¤ A: æ­£åœ¨è¯»å–æ‰€æœ‰åˆ†ææŠ¥å‘Š..."})
        report_keys = ["scoring", "business", "technical", "pricing"]
        report_contents: Dict[str, str] = {}
        async with mcp_client:
            tasks = [mcp_read_file(mcp_client, OUTPUT_PATHS[key]) for key in report_keys]
            results = await asyncio.gather(*tasks)
            
            for key, content in zip(report_keys, results):
                if content is None:
                    yield sse("warning", {"phase": "æœ€ç»ˆæ¸…å•æ•´åˆ", "text": f"è­¦å‘Šï¼šæœªæ‰¾åˆ°åˆ†ææŠ¥å‘Š '{OUTPUT_PATHS[key]}'ï¼Œè¯¥éƒ¨åˆ†å¯èƒ½ä¸å®Œæ•´ã€‚"})
                    report_contents[key] = ""
                else:
                    report_contents[key] = content
        
        if not report_contents.get("scoring"):
            yield sse("error", {"type": "MissingInputError", "message": "æ— æ³•è¿›è¡Œæ¸…å•æ•´åˆï¼Œå› ä¸ºè¯„åˆ†æŠ¥å‘Šç¼ºå¤±ã€‚"})
            return

        # --- æ­¥éª¤ 7.2: æ„å»ºâ€œè¯„åˆ†é©±åŠ¨â€çš„å¤§çº² ---
        yield sse("note", {"phase": "æœ€ç»ˆæ¸…å•æ•´åˆ", "text": "æ­¥éª¤ B: æ­£åœ¨æ„å»ºâ€œè¯„åˆ†é©±åŠ¨â€çš„æ¸…å•å¤§çº²..."})
        outline_agent = checklist_outline_agent(language=language)
        outline_result = await Runner.run(
            outline_agent,
            f"è¯·æ ¹æ®ä»¥ä¸‹è¯„åˆ†è¦æ±‚æ–‡æ¡£ï¼Œåˆ›å»ºæ¸…å•å¤§çº²ï¼š\n\n---\n\n{report_contents['scoring']}",
            run_config=run_config
        )
        checklist_outline = outline_result.final_output
        
        # ä¿å­˜å¹¶æ¨é€å¤§çº²äº§ç‰©
        outline_filename = "checklist_outline.md"
        async with mcp_client:
            await mcp_smart_write(mcp_client, outline_filename, checklist_outline)
        yield sse("artifact", {"type": "file", "filename": outline_filename})
        yield sse("note", {"phase": "æœ€ç»ˆæ¸…å•æ•´åˆ", "text": "æ¸…å•å¤§çº²æ„å»ºå®Œæˆã€‚"})

        # --- æ­¥éª¤ 7.3: æ‹†åˆ†å¤§çº²å¹¶åˆ†ä¸‰æ¬¡å¯ŒåŒ– ---
        yield sse("note", {"phase": "æœ€ç»ˆæ¸…å•æ•´åˆ", "text": "æ­¥éª¤ C: æ­£åœ¨åˆ†ä¸‰æ¬¡ã€é€éƒ¨åˆ†åœ°å¡«å……å¤§çº²..."})
        outline_sections = split_outline_by_headings(checklist_outline)
        enrichment_agent = checklist_enrichment_agent(language=language)
        final_checklist_parts: List[str] = []

        process_order = ["business", "technical", "pricing"]
        for section_key in process_order:
            if section_key in outline_sections:
                yield sse("update", {"phase": "æœ€ç»ˆæ¸…å•æ•´åˆ", "progress": f"æ­£åœ¨å¤„ç† {section_key} éƒ¨åˆ†..."})
                
                outline_part = outline_sections[section_key]
                report_content = report_contents.get(section_key, 'æ— ç›¸å…³æŠ¥å‘Šå†…å®¹')

                full_context_input = (
                    f"# æ ¸å¿ƒè¡ŒåŠ¨å¤§çº² (å½“å‰éƒ¨åˆ†)\n\n{outline_part}\n\n"
                    f"---\n\n# è¯¦ç»†éœ€æ±‚æŠ¥å‘Š (å¯¹åº”éƒ¨åˆ†)\n\n{report_content}"
                )
                
                enriched_part_result = await Runner.run(
                    enrichment_agent, full_context_input, run_config=run_config
                )
                enriched_part = enriched_part_result.final_output
                final_checklist_parts.append(enriched_part)
                yield sse("note", {"phase": "æœ€ç»ˆæ¸…å•æ•´åˆ", "text": f"{section_key} éƒ¨åˆ†å¡«å……å®Œæˆã€‚"})

        # --- æ­¥éª¤ 7.4: åˆå¹¶å¹¶ä¿å­˜æœ€ç»ˆæ¸…å• ---
        yield sse("note", {"phase": "æœ€ç»ˆæ¸…å•æ•´åˆ", "text": "æ­¥éª¤ D: æ­£åœ¨åˆå¹¶å¹¶ä¿å­˜æœ€ç»ˆæ¸…å•..."})
        final_checklist = "\n\n---\n\n".join(final_checklist_parts)
        final_checklist_filename = "final_checklist.md"
        
        async with mcp_client:
            await mcp_smart_write(mcp_client, final_checklist_filename, final_checklist)
        
        yield sse("artifact", {"type": "file", "filename": final_checklist_filename})
        yield sse("phase_end", {"name": "æœ€ç»ˆæ¸…å•æ•´åˆ"})

        # --- æµæ°´çº¿ç»“æŸ ---
        yield sse("complete", {"final_output": "æ‰€æœ‰åˆ†æé˜¶æ®µå‡å·²å®Œæˆï¼"})

    except Exception as e:
        error_info = {"type": type(e).__name__, "message": str(e)}
        yield sse("error", error_info)
